---
title: "Building a Simple Neural Network in R"
output: pdf_document
sansfont: Calibri Light
documentclass: article
header-includes:
- \usepackage{booktabs}
- \usepackage{graphicx}
- \usepackage{svg}
- \usepackage{xcolor}
---

```{r, echo=FALSE, message=FALSE}
# Load libraries and read data
library(DiagrammeR)
library(cerebrum)
library(ggplot2)
library(purrr)
library(pdata)
library(magrittr)
```

# Introduction
The idea of this report is to try and build up a neural network from scratch in R, gaining an understanding of how the algorithm works and how it is used to predict unseen data. We will try to avoid much of the mathematical concepts in a great deal of rigour, and attempt to provide the concept through doing. There are some excellent documents and explanations of why things are the way they are online, and this is not the purpose of this report.

So, what is a neural network and why should I care? It is a system made up of multiple neurons, where a neuron can be thought of simply as some function. In the sense that a neuron takes some input, processes that information in a way, and spits out its result based on how it regards that input.

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics(path = "./images/neural_neuron.pdf")
```

As mentioned earlier, a neural network is comprised of these _neurons_, in multiple layers. Therefore they can be arranged in multiple ways, for example;

```{r echo=FALSE, out.width='40%', fig.align='center'}
knitr::include_graphics(path = "./images/simple.pdf")
```

# Theory and Equations
Before we go any further, let us develop some key points and notions that will help us understand at least what is happening when building and learning a neural network.

In order to calculate the activations for _each_ of the neurons (not including the input layer - the activations here are just the inputs $\mathbf{x}$), we define the following, for every layer 
$$
a^{i}_{j} = \sigma\Big(\sum^{l_{i}}_{k=1}w^{i}_{jk}a^{i-1}_{k} + b^{i}_{j}\Big) ~~~~~~~~ \forall ~ i \in [l_2, ..., l_N]
$$
where $\sigma$ is known as the activation function, which for the purposes here we will choose to be a sigmoid function defined as,
$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$
The summation above is over the number of neurons in each layer, which will hopefully make sense when covering the worked example. It is simply just the product of all the weights and their respective activations of the previous layer, with an additional term to account for the bias of each neuron. So the activations, as stated above, for layer 2 are calculated using the input vector from the input layer, i.e. $a^{1}_{j} \equiv x_{j}$.

```{r, echo=FALSE, out.width='75%', fig.align='center'}
inp <- seq(from = -10, to = 10, by = 0.001)
df <- data.frame(x = inp, y = inp %>% cerebrum::sig(), stringsAsFactors = F)

# Plot the sigma function
g <- ggplot2::ggplot(data = df, ggplot2::aes(x, y)) %>%
  `+`(ggplot2::geom_line())
g
```

Once we calculate all the activations at each neuron, the most important will be those that are in the output layer. The highest activation means that neuron will be firing for a given set of input parameters. In order to measure the accuracy, a cost function is introduced, which is defined as,
$$
C(x) = \frac{1}{2n}\sum_{i = 1}^{T} || y(x) - a^{L}(x)||^{2}
$$
which is really just a mean squared error function. Therefore the result is really a comparison between the actual output values and the final activation layer output values. This gives us some idea of the error of our neural network and provides some feedback mechanism for optimizing at a later stage. The summation is over all training data ($T$), and the cost is a function of the input parameters for each training data.



# Worked Example
We initiate a neural network with 4 layers, with 2 input neurons and 2 output neurons. The hidden layers consist of 3 and 4 layers. We will introduce the weights and biases at each stage of the calculation, otherwise the network would look overwhelming.

Now that we have introduced some ideas and terminology, we will propagate through the neural network by example for a single iteration, to see what is actually happening when we scale this up. Hopefully after going through the example, the equations in the previous section will begin to make more sense, at least in terms of what they mean in the bigger picture. We start by taking the definition of $a^{i}_{j}$, and calculate these for layers 2, 3, and 4, where we define the first layer as the input layer (usually the input of your data set), the last layer as the output layer (for example, the number of output neurons can be the number of classes during classification of your data). And finally, any 

\newpage 

## Activations (Feedforward)
### Layer 2
```{r echo=FALSE, out.width='80%', fig.align='center'}
knitr::include_graphics(path = "./images/neural_part2.pdf")
```

$$
\begin{split}
a^{2}_{1} = \sigma(\textcolor{blue}{(0.2 \times 1) + (0.4 \times 3) + 0.4}) = \sigma(1.8) \approx 0.8581 \\
a^{2}_{2} = \sigma(\textcolor{red}{(0.1 \times 1) + (0.8 \times 3) + 0.8}) = \sigma(3.3) \approx 0.9644 \\
a^{2}_{3} = \sigma(\textcolor{green}{(0.3 \times 1) + (0.1 \times 3) + 0.1}) = \sigma(0.7) \approx 0.6682
\end{split}
$$

### Layer 3
```{r echo=FALSE, out.width='80%', fig.align='center'}
knitr::include_graphics(path = "./images/neural_part3.pdf")
```

$$
\begin{split}
a^{3}_{1} = \sigma(\textcolor{blue}{(0.3 \times 0.8581) + (0.6 \times 0.9644) + (0.5 \times 0.6682) + 0.2}) = \sigma(1.3702) \approx 0.7974 \\
a^{3}_{2} = \sigma(\textcolor{red}{(0.1 \times 0.8581) + (0.1 \times 0.9644) + (0.4 \times 0.6682) + 0.4}) = \sigma(0.8495) \approx 0.7005 \\
a^{3}_{3} = \sigma(\textcolor{green}{(0.9 \times 0.8581) + (0.2 \times 0.9644) + (0.2 \times 0.6682) + 0.1}) = \sigma(1.1988) \approx 0.7683 \\
a^{3}_{4} = \sigma(\textcolor{orange}{(0.7 \times 0.8581) + (0.3 \times 0.9644) + (0.8 \times 0.6682) + 0.8}) = \sigma(2.2246) \approx 0.9024
\end{split}
$$

### Layer 4
```{r echo=FALSE, out.width='80%', fig.align='center'}
knitr::include_graphics(path = "./images/neural_part4.pdf")
```

$$
\begin{split}
a^{4}_{1} = \sigma(\textcolor{blue}{(0.3 \times 0.7974) + (0.1 \times 0.7005) + (0.4 \times 0.7683) + (0.2 \times 0.9024) + 0.1}) = \sigma(0.8971) \approx 0.7103 \\
a^{4}_{2} = \sigma(\textcolor{red}{(0.6 \times 0.7974) + (0.9 \times 0.7005) + (0.7 \times 0.7683) + (0.7 \times 0.9024) + 0.6}) = \sigma(0.9468) \approx 0.9468
\end{split}
$$

## Backpropagation
Now that we have calculated all the activations at each neuron for a single iteration, we can finally apply the backpropagation algorithm to calculate the delta' and therefore any changes that need to be made to the weights and biases in order to reach the desired output vectors for each corresponding piece of input.

The following work and equations can be used for the cost function, $C(x)$ as defined before. 

### Kickstarting the algorithm
To start the algorithm, we calculate the first value, $\delta^{L}$, where we are using our total number of layers as $L = 4$.
$$
\delta^{4} = (a^{4} - o) \odot \sigma^{\prime} (z^4) =
\Big(\begin{bmatrix} 0.7104 \\ 0.9468 \end{bmatrix} - \begin{bmatrix} 0 \\ 1 \end{bmatrix} \Big) \odot \sigma'\Big(\begin{bmatrix} 0.8971 \\ 2.8784 \end{bmatrix}\Big) =
\begin{bmatrix} 0.1462 \\ -0.002681 \end{bmatrix}
$$
This gives us the ability to calculate the changes in the biases and weights,
$$
\frac{\partial C}{\partial b^{4}} = \begin{bmatrix} 0.1462 \\ -0.002681 \end{bmatrix}
$$

$$
\frac{\partial C}{\partial w^{4}} = \delta^4 \cdot (a^{3})^{\top} = \begin{bmatrix} 0.1165 & 0.1024 & 0.1123 & 0.1319 \\ -0.00214 & -0.00187 & -0.00206 & -0.00242 \end{bmatrix}
$$
### Remaining Layers
Now, by using the the $\delta$ recurrence relation, linking delta's in adjacent layers, we can use $\delta^4$ from above and calculate the remaining changes in the weights and biases. The calculation is as follows;

$$
...
$$
