---
title: "Building a Simple Neural Network in R"
output: pdf_document
sansfont: Calibri Light
documentclass: article
header-includes:
- \usepackage{booktabs}
- \usepackage{graphicx}
- \usepackage{svg}
---

```{r, echo=FALSE, message=FALSE}
# Load libraries and read data
library(DiagrammeR)
library(cerebrum)
library(purrr)
library(magrittr)
```

# Introduction
The idea of this report is to try and build up a neural network from scratch in R, gaining an understanding of how the algorithm works and how it is used to predict unseen data. We will try to avoid much of the mathematical concepts in a great deal of rigour, and attempt to provide the concept through doing. There are some excellent documents and explanations of why things are the way they are online, and this is not the purpose of this report.

So, what is a neural network and why should I care? It is a system made up of multiple neurons, where a neuron can be thought of simply as some function. In the sense that a neuron takes some input, processes that information in a way, and spits out its result based on how it regards that input.

```{r echo=FALSE, out.width='100%'}
knitr::include_graphics(path = "./images/neural_neuron.pdf")
```

As mentioned earlier, a neural network is comprised of these _neurons_, in multiple layers. Therefore they can be arranged in multiple ways, for example;

```{r echo=FALSE, out.width='100%'}
print('image here')
#knitr::include_graphics(path = "./images/simple.pdf")
```

# Theory and Equations
Before we go any further, let us develop some key points and notions that will help us understand at least what is happening when building and learning a neural network.

In order to calculate the activations for _each_ of the neurons (not including the input layer - the activations here are just the inputs $\mathbf{x}$), we define the following, for every layer 
$$
a^{i}_{j} = \sigma\Big(\sum^{l_{i}}_{k=1}w^{i}_{jk}a^{i-1}_{k} + b^{i}_{j}\Big) ~~~~~~~~ \forall ~ i \in [l_2, ..., l_N]
$$
where $\sigma$ is known as the activation function, which for the purposes here we will choose to be a sigmoid function defined as,
$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$
The summation above is over the number of neurons in each layer, which will hopefully make sense when covering the worked example. It is simply just the product of all the weights and their respective activations of the previous layer, with an additional term to account for the bias of each neuron. So the activations, as stated above, for layer 2 are calculated using the input vector from the input layer, i.e. $a^{1}_{j} \equiv x_{j}$.

# Worked Example
Now that we have introduced some ideas and terminology, we will propagate through the neural network by example for a single iteration, to see what is actually happening when we scale this up. Hopefully after going through the example, the equations in the previous section will begin to make more sense, at least in terms of what they mean in the bigger picture.

### Activation functions
$$
\begin{split}
a^{2}_{1} = \sigma((0.2)(1) + (0.4)(3) + 0.4) = \sigma(1.8) \approx 0.8581 \\
a^{2}_{2} = \sigma((0.1)(1) + (0.8)(3) + 0.8) = \sigma(3.3) \approx 0.9644 \\
a^{2}_{3} = \sigma((0.3)(1) + (0.1)(3) + 0.1) = \sigma(0.7) \approx 0.6682
\end{split}
$$

where we define the first layer as the input layer (usually the input of your data set), the last layer as the output layer (for example, the number of output neurons can be the number of classes during classification of your data). And finally, any 


```{r echo=FALSE, out.width='50%'}
print('image here')
#knitr::include_graphics(path = "./images/neural_part3.pdf")
```

```{r echo=FALSE, out.width='50%'}
print('image here')
#knitr::include_graphics(path = "./images/neural_part4.pdf")
```
